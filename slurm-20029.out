['tools/..', '/data/run01/scz0088/project/AutoRF/tools', '/HOME/scz0088/.conda/envs/hb/lib/python37.zip', '/HOME/scz0088/.conda/envs/hb/lib/python3.7', '/HOME/scz0088/.conda/envs/hb/lib/python3.7/lib-dynload', '/HOME/scz0088/.conda/envs/hb/lib/python3.7/site-packages', '/HOME/scz0088/.conda/envs/hb/lib/python3.7/site-packages/openselfsup-0.3.0+ab8fc27-py3.7.egg', '/HOME/scz0088/.conda/envs/hb/lib/python3.7/site-packages/wheel-0.37.0-py3.7.egg', '/data/run01/scz0088/.conda/envs/hb/lib/python3.7/site-packages']
Experiment dir : exps/search/rf_p1-EXP-20220126-214040
01/26 09:40:40 PM gpu device = 0
01/26 09:40:40 PM args = Namespace(arch_learning_rate=0.003, arch_weight_decay=0.001, batch_size=128, cutout=False, cutout_length=16, data='/data/public/cifar', drop_path_prob=0.3, epochs=100, gpu=0, grad_clip=5, init_channels=16, layers=8, learning_rate=0.025, learning_rate_min=0.0001, model_name='rf_p1', model_path='saved_models', momentum=0.9, report_freq=50, save='exps/search/rf_p1-EXP-20220126-214040', seed=2, train_portion=0.5, unrolled=False, weight_decay=0.0005)
01/26 09:40:51 PM param size = 0.405530MB
Files already downloaded and verified
01/26 09:40:52 PM epoch 0 lr 2.498771e-02
01/26 09:40:52 PM genotype = Genotype(normal=[('avg_pool_5x5', 0), ('noise', 1), ('avg_pool_3x3', 0), ('max_pool_5x5', 0), ('avg_pool_7x7', 1), ('avg_pool_7x7', 2)], normal_concat=range(0, 4))
tensor([[0.1253, 0.1249, 0.1249, 0.1249, 0.1249, 0.1249, 0.1251, 0.1250],
        [0.1249, 0.1251, 0.1248, 0.1251, 0.1249, 0.1252, 0.1250, 0.1250],
        [0.1250, 0.1252, 0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1248],
        [0.1249, 0.1249, 0.1251, 0.1252, 0.1250, 0.1249, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1248, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1248, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
/HOME/scz0088/.conda/envs/hb/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
/HOME/scz0088/.conda/envs/hb/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/HOME/scz0088/.conda/envs/hb/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
01/26 09:41:00 PM train 000 2.353413e+00 7.812500 53.125000
01/26 09:42:51 PM train 050 2.002104e+00 25.291054 76.930147
01/26 09:44:42 PM train 100 1.856633e+00 30.290842 82.410272
01/26 09:46:33 PM train 150 1.757265e+00 33.992136 85.068295
01/26 09:48:12 PM train_acc 36.716000
01/26 09:48:13 PM valid 000 1.502590e+00 44.531250 90.625000
01/26 09:48:18 PM valid 050 1.492760e+00 46.446078 91.835172
01/26 09:48:23 PM valid 100 1.492375e+00 46.565594 91.847153
01/26 09:48:27 PM valid 150 1.497656e+00 46.129967 92.011589
01/26 09:48:32 PM valid_acc 45.808000
01/26 09:48:32 PM epoch 1 lr 2.495702e-02
01/26 09:48:32 PM genotype = Genotype(normal=[('max_pool_7x7', 0), ('max_pool_7x7', 0), ('avg_pool_7x7', 1), ('avg_pool_5x5', 2), ('max_pool_7x7', 0), ('max_pool_7x7', 1)], normal_concat=range(0, 4))
tensor([[0.1207, 0.0986, 0.1268, 0.1473, 0.1539, 0.1188, 0.1048, 0.1290],
        [0.1299, 0.1081, 0.1247, 0.1478, 0.1535, 0.1182, 0.1176, 0.1003],
        [0.1037, 0.0994, 0.1250, 0.1390, 0.1228, 0.1354, 0.1347, 0.1400],
        [0.1216, 0.1141, 0.1203, 0.1322, 0.1519, 0.1235, 0.1203, 0.1160],
        [0.1078, 0.1026, 0.1221, 0.1452, 0.1466, 0.1269, 0.1224, 0.1265],
        [0.1092, 0.1039, 0.1206, 0.1325, 0.1210, 0.1248, 0.1595, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tools/search.py:235: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  input = Variable(input, volatile=True).cuda()
tools/search.py:236: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  target = Variable(target, volatile=True).cuda()
01/26 09:48:36 PM train 000 1.357332e+00 46.093750 95.312500
01/26 09:50:29 PM train 050 1.384865e+00 49.341299 93.321078
slurmstepd: error: *** JOB 20029 ON g0005 CANCELLED AT 2022-01-26T21:51:09 ***
